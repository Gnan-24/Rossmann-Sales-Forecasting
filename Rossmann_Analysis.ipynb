{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rossmann Sales Forecasting: A Data-Driven Approach\n",
    "\n",
    "### 1. Project Objective\n",
    "\n",
    "The goal of this project is to provide Rossmann, a major European drug store chain, with a robust tool to accurately forecast daily sales for their stores up to six weeks in advance. Accurate sales forecasting is critical for business operations, enabling:\n",
    "\n",
    "* **Optimized Staffing:** Scheduling the right number of employees to meet customer demand without overspending on labor.\n",
    "* **Efficient Inventory Management:** Ensuring popular products are in stock while minimizing excess inventory and associated holding costs.\n",
    "* **Strategic Promotion Planning:** Understanding the true impact of promotional events to maximize their return on investment.\n",
    "\n",
    "This notebook will walk through the entire data science workflow: from data cleaning and exploratory analysis to feature engineering and predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup and Data Loading\n",
    "\n",
    "First, we'll import the necessary libraries for data manipulation, visualization, and modeling. Then, we'll load the provided datasets: `train.csv`, `store.csv`, and `test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('train.csv', low_memory=False, parse_dates=['Date'])\n",
    "store_df = pd.read_csv('store.csv', low_memory=False)\n",
    "test_df = pd.read_csv('test.csv', low_memory=False, parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Cleaning and Merging\n",
    "\n",
    "Before analysis, we need to understand our data's structure and handle any inconsistencies or missing values. We will then merge the store-specific information with our training and testing dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (1017209, 9)\n",
      "Store Data Shape: (1115, 10)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Data Shape: {train_df.shape}\")\n",
    "print(f\"Store Data Shape: {store_df.shape}\")\n",
    "\n",
    "# Merge dataframes\n",
    "df = pd.merge(train_df, store_df, on='Store', how='left')\n",
    "test_df = pd.merge(test_df, store_df, on='Store', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Missing Values\n",
    "\n",
    "Let's investigate the missing values. A large number of missing values in `CompetitionDistance` and `Promo2` related columns suggests we need a clear strategy for imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>...</div>",
      "text/plain": "..."
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() / len(df) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imputation Strategy:**\n",
    "* **CompetitionDistance:** A missing value could imply there is no nearby competition. We will fill these with a very large number to signify a great distance, which is more informative than zero or the mean.\n",
    "* **Other Competition Columns:** Missing values here are linked to the absence of competition. We'll fill these with 0.\n",
    "* **Promo2 Columns:** Missing values indicate the store is not participating in the continuous promotion `Promo2`. We'll fill these with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill CompetitionDistance with a large value\n",
    "df['CompetitionDistance'].fillna(df['CompetitionDistance'].max(), inplace=True)\n",
    "test_df['CompetitionDistance'].fillna(test_df['CompetitionDistance'].max(), inplace=True)\n",
    "\n",
    "# Fill other missing values with 0\n",
    "df.fillna(0, inplace=True)\n",
    "test_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Now, we dive deep into the data to uncover patterns, validate assumptions, and understand the relationships between different variables and our target, `Sales`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the Target Variable: Sales\n",
    "\n",
    "First, let's examine the distribution of sales. We see a significant spike at `Sales = 0`. This is a critical insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "...",
      "text/plain": "<Axes: xlabel='Sales', ylabel='Count'>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(df['Sales'], bins=50, kde=False)\n",
    "plt.title('Distribution of Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:** The spike at zero almost certainly corresponds to days when the stores were closed. Forecasting for closed stores is unnecessary. Therefore, we will filter our dataset to only include days when stores were open and `Sales > 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['Open'] == 1) & (df['Sales'] > 0)]"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do Sales vary by Store Type and Assortment?\n",
    "\n",
    "We can see clear differences in performance. `StoreType 'b'` is the strongest performer on average, and stores with an `'extended'` assortment (`Assortment 'b'`) also show higher sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "...",
      "text/plain": "<Axes: >"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "sns.boxplot(data=df, x='StoreType', y='Sales', ax=ax1)\n",
    "ax1.set_title('Sales by Store Type')\n",
    "\n",
    "sns.boxplot(data=df, x='Assortment', y='Sales', ax=ax2)\n",
    "ax2.set_title('Sales by Assortment Level')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impact of Promotions and Holidays\n",
    "\n",
    "As expected, sales are significantly higher on days with promotions (`Promo = 1`). State holidays, however, drive sales to zero as stores are closed. We will engineer features to capture these effects more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "...",
      "text/plain": "<Axes: >"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "sns.barplot(data=df, x='Promo', y='Sales', ax=ax1)\n",
    "ax1.set_title('Average Sales with and without Promotion')\n",
    "\n",
    "sns.barplot(data=df, x='StateHoliday', y='Sales', ax=ax2)\n",
    "ax2.set_title('Average Sales by Holiday Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Feature Engineering\n",
    "\n",
    "This is the most critical step for improving model performance. We will extract valuable information from existing columns, especially the `Date` column, to create new, powerful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(data):\n",
    "    # Date features\n",
    "    data['Year'] = data.Date.dt.year\n",
    "    data['Month'] = data.Date.dt.month\n",
    "    data['Day'] = data.Date.dt.day\n",
    "    data['WeekOfYear'] = data.Date.dt.isocalendar().week.astype(int)\n",
    "\n",
    "    # Competition features\n",
    "    data['CompetitionOpenSince'] = (data['Year'] - data['CompetitionOpenSinceYear']) * 12 + \\\n",
    "                                      (data['Month'] - data['CompetitionOpenSinceMonth'])\n",
    "    data['CompetitionOpenSince'] = data['CompetitionOpenSince'].apply(lambda x: max(0, x))\n",
    "    \n",
    "    # Promotion features\n",
    "    data['Promo2Since'] = (data['Year'] - data['Promo2SinceYear']) * 52 + \\\n",
    "                          (data['WeekOfYear'] - data['Promo2SinceWeek'])\n",
    "    data['Promo2Since'] = data['Promo2Since'].apply(lambda x: max(0, x))\n",
    "    \n",
    "    return data\n",
    "\n",
    "df = feature_engineer(df)\n",
    "test_df = feature_engineer(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data Preprocessing and Modeling\n",
    "\n",
    "We'll now prepare the data for our models. This involves selecting relevant features, encoding categorical variables, and splitting our data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables to numerical codes\n",
    "categorical_cols = ['StoreType', 'Assortment', 'StateHoliday']\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].astype('category').cat.codes\n",
    "    test_df[col] = test_df[col].astype('category').cat.codes\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "features = [\n",
    "    'Store', 'DayOfWeek', 'Promo', 'StateHoliday', 'SchoolHoliday',\n",
    "    'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSince',\n",
    "    'Promo2', 'Promo2Since', 'Year', 'Month', 'Day', 'WeekOfYear'\n",
    "]\n",
    "target = 'Sales'\n",
    "\n",
    "# Split data for validation\n",
    "# We will use the last 6 weeks of data for validation, mirroring the test set\n",
    "split_date = df['Date'].max() - pd.DateOffset(weeks=6)\n",
    "train_data = df[df['Date'] < split_date]\n",
    "val_data = df[df['Date'] >= split_date]\n",
    "\n",
    "X_train, y_train = train_data[features], np.log1p(train_data[target])\n",
    "X_val, y_val = val_data[features], np.log1p(val_data[target])"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metric: RMSPE\n",
    "\n",
    "The competition uses the Root Mean Square Percentage Error (RMSPE). We will define a function for this. We model the logarithm of sales (`log1p`) to handle its skewed distribution, so we must remember to convert predictions back (`expm1`) before calculating the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rmspe(y_true, y_pred):\n",
    "    # Convert from log scale\n",
    "    y_true_orig = np.expm1(y_true)\n",
    "    y_pred_orig = np.expm1(y_pred)\n",
    "    \n",
    "    # Calculate RMSPE\n",
    "    # Ensure no division by zero for closed stores (already filtered, but good practice)\n",
    "    percentage_error = (y_true_orig - y_pred_orig) / y_true_orig\n",
    "    return np.sqrt(np.mean(percentage_error ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Random Forest (Baseline)\n",
    "\n",
    "Let's start with a strong baseline model, the Random Forest Regressor. It's robust and gives us a good sense of feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n",
      "Random Forest Validation RMSPE: 0.1385\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_preds = rf_model.predict(X_val)\n",
    "rf_rmspe = to_rmspe(y_val, rf_preds)\n",
    "print(f\"Random Forest Validation RMSPE: {rf_rmspe:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: XGBoost (Advanced Model)\n",
    "\n",
    "Now, we'll use XGBoost, a gradient boosting algorithm known for its high performance in competitions. It often provides a significant lift over Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost...\n",
      "XGBoost Validation RMSPE: 0.1152\n"
     ]
    }
   ],
   "source": [
    "print(\"Training XGBoost...\")\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train, \n",
    "              eval_set=[(X_val, y_val)], \n",
    "              eval_metric='rmse', \n",
    "              early_stopping_rounds=50, \n",
    "              verbose=False)\n",
    "\n",
    "xgb_preds = xgb_model.predict(X_val)\n",
    "xgb_rmspe = to_rmspe(y_val, xgb_preds)\n",
    "print(f\"XGBoost Validation RMSPE: {xgb_rmspe:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Feature Importance & Conclusion\n",
    "\n",
    "Let's examine what our best model (XGBoost) learned. Understanding which features drive the forecast is a key part of delivering a data science solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "...",
      "text/plain": "<Axes: >"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "xgb.plot_importance(xgb_model, ax=ax, height=0.8, max_num_features=15)\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Conclusions\n",
    "\n",
    "* **Model Performance:** Our final XGBoost model achieved a validation **RMSPE of 0.1152**, a significant improvement over the Random Forest baseline. This indicates our model can forecast sales with a relatively high degree of accuracy.\n",
    "\n",
    "* **Key Drivers of Sales:** The most important features for predicting sales are:\n",
    "    1.  **Store:** Inherent store-level properties are the single biggest predictor.\n",
    "    2.  **CompetitionDistance & CompetitionOpenSince:** The presence, distance, and maturity of competitors heavily influence sales.\n",
    "    3.  **Promo:** Promotions provide a major, predictable lift in sales.\n",
    "    4.  **Temporal Features:** The `WeekOfYear`, `DayOfWeek`, and `Month` all capture crucial seasonal patterns.\n",
    "\n",
    "* **Business Recommendations:**\n",
    "    * The model can be deployed to provide store managers with reliable sales forecasts for optimizing staffing and inventory.\n",
    "    * The feature importance analysis confirms that the `Promo` strategy is highly effective. The model could be used to simulate the potential uplift of future promotions.\n",
    "    * Store performance is heavily impacted by competitor locations. The model could be a valuable tool for market analysis when considering new store locations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}